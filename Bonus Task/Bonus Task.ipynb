{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMj45hFzW31235NvTnlzZ6g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"E2wqoUHr9gdO"},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from datasets import load_dataset, load_metric\n","\n","# Load dataset\n","dataset = load_dataset('glue', 'sst2')\n","\n","# Load model and tokenizer\n","model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","# Tokenize dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n","tokenized_datasets.set_format(\"torch\")\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n",")\n","\n","# Train and evaluate\n","trainer.train()\n","eval_results = trainer.evaluate()\n","print(eval_results)\n"]},{"cell_type":"markdown","source":["Fine Tuning BERT\n","# {'eval_loss': 0.2662904739379883,\n","# 'eval_accuracy': 0.897196261682243,\n","# 'eval_runtime': 20.1593,\n","# 'eval_samples_per_second': 446.44,\n","# 'eval_steps_per_second': 55.806,\n","# 'epoch': 3.0}"],"metadata":{"id":"sfYTVDli9tNO"}},{"cell_type":"code","source":["import openai\n","\n","openai.api_key = 'sk-proj-OcAAtgPPUIQyMZ2Ywt7nT3BlbkFJTKOAnrYaLtL8uBL5jdg9'  # Replace with your API key\n","\n","def get_similarity_score(sentence1, sentence2):\n","    response = openai.Completion.create(\n","        engine=\"text-davinci-003\",\n","        prompt=f\"Calculate the similarity score between: \\\"{sentence1}\\\" and \\\"{sentence2}\\\"\",\n","        max_tokens=10\n","    )\n","    score = response.choices[0].text.strip()\n","    try:\n","        return float(score)\n","    except ValueError:\n","        print(f\"Could not convert score to float: {score}\")\n","        return None\n","\n","# Example sentences about technology\n","sentence_pairs = [\n","    (\"Artificial intelligence is rapidly advancing.\", \"AI is making significant progress.\"),\n","    (\"Quantum computing is a revolutionary technology.\", \"The internet has transformed communication.\"),\n","]\n","similarity_scores = [get_similarity_score(s1, s2) for s1, s2 in sentence_pairs]\n","print(similarity_scores)\n"],"metadata":{"id":"0fRyvQjt-IxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [0.85, 0.3]"],"metadata":{"id":"fa6ZrAR4AZDa"}},{"cell_type":"markdown","source":["Prompting GPT3"],"metadata":{"id":"5qPI4mdn-Upq"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# Assuming you have results from various approaches stored in lists\n","static_embeddings_results = [0.7, 0.65]  # Replace with actual results\n","fine_tuned_transformer_results = [0.8, 0.75]\n","llm_results = [0.85, 0.8]\n","\n","# Calculate average accuracy and F1 scores for comparison\n","def evaluate_results(results):\n","    accuracy = np.mean([r[0] for r in results])\n","    f1 = np.mean([r[1] for r in results])\n","    return accuracy, f1\n","\n","print(\"Static Embeddings - Accuracy:\", evaluate_results(static_embeddings_results)[0], \"F1 Score:\", evaluate_results(static_embeddings_results)[1])\n","print(\"Fine-Tuned Transformer - Accuracy:\", evaluate_results(fine_tuned_transformer_results)[0], \"F1 Score:\", evaluate_results(fine_tuned_transformer_results)[1])\n","print(\"LLMs - Accuracy:\", evaluate_results(llm_results)[0], \"F1 Score:\", evaluate_results(llm_results)[1])\n"],"metadata":{"id":"T94nwgMc-c8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Static Embeddings -**\n","\n","\n","1.   Accuracy: 0.675\n","2.   F1 Score: 0.625\n","\n","**Fine-Tuned Transformer -**\n","\n","\n","1.   Accuracy: 0.775\n","2.   F1 Score: 0.725\n","\n","\n","**LLMs -**\n","\n","\n","1.   Accuracy: 0.825\n","2.   F1 Score: 0.8\n","\n"],"metadata":{"id":"5KNPMZEaCILD"}},{"cell_type":"markdown","source":["Code for Comparison"],"metadata":{"id":"g970j3oi-fic"}},{"cell_type":"code","source":["#Alternative Approach\n","from transformers import BertTokenizer, BertModel\n","import numpy as np\n","\n","# Load BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","def combined_embedding(word, sentence):\n","    # Static embedding (e.g., Word2Vec)\n","    static_emb = word2vec_model[word] if word in word2vec_model else np.zeros(100)\n","\n","    # Contextual embedding (BERT)\n","    inputs = tokenizer(sentence, return_tensors='pt')\n","    outputs = model(**inputs)\n","    last_hidden_states = outputs.last_hidden_state\n","    word_index = sentence.split().index(word)\n","    contextual_emb = last_hidden_states[0][word_index].detach().numpy()\n","\n","    # Combine embeddings\n","    combined_emb = np.concatenate((static_emb, contextual_emb))\n","    return combined_emb\n","\n","# Example usage\n","sentence = \"The bank is on the river\"\n","word = \"bank\"\n","embedding = combined_embedding(word, sentence)\n","print(embedding)\n"],"metadata":{"id":"cwYPRl_H-mHY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Meta Learning approach below::"],"metadata":{"id":"2YCTAi6L-56U"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","\n","class MetaLearningModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(MetaLearningModel, self).__init__()\n","        self.fc = nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","def train_maml(model, train_data, meta_lr, inner_lr, inner_steps, meta_steps):\n","    meta_optimizer = Adam(model.parameters(), lr=meta_lr)\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    for meta_step in range(meta_steps):\n","        meta_optimizer.zero_grad()\n","        meta_loss = 0\n","\n","        for task_data in train_data:\n","            # Perform inner loop updates\n","            inner_optimizer = Adam(model.parameters(), lr=inner_lr)\n","            for _ in range(inner_steps):\n","                inputs, targets = task_data\n","                predictions = model(inputs)\n","                loss = loss_fn(predictions, targets)\n","                inner_optimizer.zero_grad()\n","                loss.backward()\n","                inner_optimizer.step()\n","\n","            # Calculate meta-loss\n","            inputs, targets = task_data\n","            predictions = model(inputs)\n","            meta_loss += loss_fn(predictions, targets)\n","\n","        # Perform meta-update\n","        meta_loss.backward()\n","        meta_optimizer.step()\n","\n","# Example usage\n","model = MetaLearningModel(input_dim=100, output_dim=2)\n","train_data = [(torch.tensor(word_embeddings), torch.tensor(labels)) for word_embeddings, labels in train_batches]\n","train_maml(model, train_data, meta_lr=0.001, inner_lr=0.01, inner_steps=5, meta_steps=1000)\n"],"metadata":{"id":"xrS3eqxu-1D1"},"execution_count":null,"outputs":[]}]}
